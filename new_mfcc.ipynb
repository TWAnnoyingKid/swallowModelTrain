{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import librosa\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import joblib\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import confusion_matrix, classification_report, roc_curve, auc\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, regularizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjust_volume(y, factor):\n",
    "    return y * factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_mfcc(file_path, n_mfcc=40, hop_length=64, fixed_length=160, sr=22050, duration=0.5, augment=False):\n",
    "    try:\n",
    "        y, sr = librosa.load(file_path, sr=sr, duration=duration)\n",
    "        y = librosa.util.fix_length(y, size=int(sr * duration))\n",
    "\n",
    "        if augment:\n",
    "            augmentation_methods = ['noise', 'pitch', 'speed']\n",
    "            chosen_method = np.random.choice(augmentation_methods)\n",
    "            if chosen_method == 'noise':\n",
    "                y = adjust_volume(y,0.8)\n",
    "            # elif chosen_method == 'speed':\n",
    "            #     speed_factor = np.random.uniform(0.8, 1.25)\n",
    "            #     y = change_speed(y, speed_factor=speed_factor)\n",
    "\n",
    "        mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=n_mfcc, hop_length=hop_length)\n",
    "        mfcc = librosa.power_to_db(mfcc, ref=np.max)\n",
    "\n",
    "        if mfcc.shape[1] < fixed_length:\n",
    "            pad_width = fixed_length - mfcc.shape[1]\n",
    "            mfcc = np.pad(mfcc, pad_width=((0,0), (0, pad_width)), mode='constant')\n",
    "        else:\n",
    "            mfcc = mfcc[:, :fixed_length]\n",
    "\n",
    "        return mfcc.astype(np.float32)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"處理文件 {file_path} 時出現錯誤: {e}\")\n",
    "        return None\n",
    "\n",
    "def extract_mfcc_from_data(y_segment, sr, n_mfcc=40, hop_length=64, fixed_length=160):\n",
    "    try:\n",
    "        # 提取 MFCC\n",
    "        mfcc = librosa.feature.mfcc(y=y_segment, sr=sr, n_mfcc=n_mfcc, hop_length=hop_length)\n",
    "        mfcc = librosa.power_to_db(mfcc, ref=np.max)\n",
    "        \n",
    "        # 填充或截斷時間步數\n",
    "        if mfcc.shape[1] < fixed_length:\n",
    "            pad_width = fixed_length - mfcc.shape[1]\n",
    "            mfcc = np.pad(mfcc, pad_width=((0,0), (0, pad_width)), mode='constant')\n",
    "        else:\n",
    "            mfcc = mfcc[:, :fixed_length]\n",
    "        \n",
    "        return mfcc.astype(np.float32)\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"處理音訊片段時出現錯誤: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(swallow_dir, non_dir, n_mfcc=40, hop_length=64, fixed_length=160, sr=22050, duration=0.5, augment=False):\n",
    "    data = []\n",
    "    labels = []\n",
    "\n",
    "    for file in os.listdir(swallow_dir):\n",
    "        if file.endswith('.wav'):\n",
    "            file_path = os.path.join(swallow_dir, file)\n",
    "            mfcc = extract_mfcc(file_path, n_mfcc=n_mfcc, hop_length=hop_length, \n",
    "                               fixed_length=fixed_length, sr=sr, duration=duration, augment=augment)\n",
    "            if mfcc is not None:\n",
    "                data.append(mfcc)\n",
    "                labels.append(1)\n",
    "\n",
    "    for file in os.listdir(non_dir):\n",
    "        if file.endswith('.wav'):\n",
    "            file_path = os.path.join(non_dir, file)\n",
    "            mfcc = extract_mfcc(file_path, n_mfcc=n_mfcc, hop_length=hop_length, \n",
    "                               fixed_length=fixed_length, sr=sr, duration=duration, augment=augment)\n",
    "            if mfcc is not None:\n",
    "                data.append(mfcc)\n",
    "                labels.append(0)\n",
    "\n",
    "    return np.array(data), np.array(labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定義資料夾路徑\n",
    "swallow_dir = 'sound_split_data/swallow'  # 替換為實際路徑\n",
    "non_dir = 'sound_split_data/non'          # 替換為實際路徑\n",
    "\n",
    "# 確認資料夾存在\n",
    "assert os.path.exists(swallow_dir), f\"資料夾 '{swallow_dir}' 不存在。請確認路徑正確。\"\n",
    "assert os.path.exists(non_dir), f\"資料夾 '{non_dir}' 不存在。請確認路徑正確。\"\n",
    "\n",
    "# 建立原始訓練資料集（不應用增強）\n",
    "X, y = create_dataset(swallow_dir, non_dir, n_mfcc=40, hop_length=64, fixed_length=160, sr=22050, duration=0.5, augment=False)\n",
    "print(f\"原始資料集大小: {X.shape}, 標籤大小: {y.shape}\")\n",
    "\n",
    "# 建立增強後的訓練資料集\n",
    "X_aug, y_aug = create_dataset(swallow_dir, non_dir, n_mfcc=40, hop_length=64, fixed_length=160, sr=22050, duration=0.5, augment=True)\n",
    "print(f\"增強後資料集大小: {X_aug.shape}, 標籤大小: {y_aug.shape}\")\n",
    "\n",
    "# 合併原始資料集與增強後的資料集\n",
    "X_combined = np.concatenate((X, X_aug), axis=0)\n",
    "y_combined = np.concatenate((y, y_aug), axis=0)\n",
    "print(f\"合併後資料集大小: {X_combined.shape}, 標籤大小: {y_combined.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 標準化\n",
    "scaler = StandardScaler()\n",
    "X_reshaped = X_combined.reshape(X_combined.shape[0], -1)  # (num_samples, 40*160=6400)\n",
    "X_scaled = scaler.fit_transform(X_reshaped)  # (num_samples, 6400)\n",
    "\n",
    "# 保存 scaler 以便測試階段使用\n",
    "joblib.dump(scaler, 'scaler/mfcc_scaler.pkl')\n",
    "print(\"Scaler 已保存為 'scaler.pkl'\")\n",
    "\n",
    "# 重塑為 (num_samples, 40, 160, 1)\n",
    "X_scaled = X_scaled.reshape(X_combined.shape[0], 40, 160, 1)\n",
    "\n",
    "# 複製通道以符合模型要求 (num_samples, 40, 160, 3)\n",
    "X_scaled = np.repeat(X_scaled, 3, axis=-1)\n",
    "print(f\"重塑後資料集大小: {X_scaled.shape}, 標籤大小: {y_combined.shape}\")\n",
    "\n",
    "# 拆分訓練集和驗證集\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_scaled, y_combined, test_size=0.2, random_state=42, stratify=y_combined\n",
    ")\n",
    "print(f\"訓練集大小: {X_train.shape}, 驗證集大小: {X_val.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_complete_regularized_cnn_lstm(input_shape):\n",
    "    inputs = layers.Input(shape=input_shape)\n",
    "\n",
    "    # 第一個卷積塊\n",
    "    x = layers.Conv2D(64, (3,3), activation='relu', padding='same',\n",
    "                      kernel_regularizer=regularizers.l2(0.001))(inputs)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.MaxPooling2D((2,2))(x)  # (20, 80, 32)\n",
    "    x = layers.Dropout(0.25)(x)\n",
    "\n",
    "    # 第二個卷積塊\n",
    "    x = layers.Conv2D(128, (3,3), activation='relu', padding='same',\n",
    "                      kernel_regularizer=regularizers.l2(0.001))(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.MaxPooling2D((2,2))(x)  # (10, 40, 64)\n",
    "    x = layers.Dropout(0.25)(x)\n",
    "\n",
    "    # 第三個卷積塊\n",
    "    x = layers.Conv2D(256, (3,3), activation='relu', padding='same',\n",
    "                      kernel_regularizer=regularizers.l2(0.001))(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.MaxPooling2D((2,2))(x)  # (5, 20, 128)\n",
    "    x = layers.Dropout(0.25)(x)\n",
    "\n",
    "    # Flatten 和 Dense\n",
    "    x = layers.Flatten()(x)  # (5*20*128, ) = (12800, )\n",
    "    x = layers.Dense(256, activation='relu', kernel_regularizer=regularizers.l2(0.001))(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Dropout(0.5)(x)\n",
    "\n",
    "    # 為 LSTM 準備序列格式\n",
    "    x = layers.Reshape((1, 256))(x)  # (1, 256)\n",
    "    x = layers.LSTM(64, return_sequences=False, dropout=0.2, recurrent_dropout=0.2,\n",
    "                   kernel_regularizer=regularizers.l2(0.001))(x)\n",
    "\n",
    "    # 輸出層\n",
    "    outputs = layers.Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "    model = models.Model(inputs, outputs)\n",
    "    return model\n",
    "\n",
    "# 定義模型輸入形狀\n",
    "input_shape = (40, 160, 3)\n",
    "\n",
    "# 建立模型\n",
    "model = build_complete_regularized_cnn_lstm(input_shape)\n",
    "\n",
    "# 編譯模型\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# 顯示模型摘要\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定義 Early Stopping 回調\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=10,\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "# 訓練模型\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=100,\n",
    "    batch_size=32,\n",
    "    validation_data=(X_val, y_val),\n",
    "    # callbacks=[early_stopping]\n",
    ")\n",
    "# 繪製訓練與驗證的準確率\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['accuracy'], label='Train_accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Val_accuracy')\n",
    "plt.legend()\n",
    "plt.title('accuracy')\n",
    "\n",
    "# 繪製訓練與驗證的損失\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['loss'], label='Train_loss')\n",
    "plt.plot(history.history['val_loss'], label='Val_loss')\n",
    "plt.legend()\n",
    "plt.title('loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 在驗證集上進行預測\n",
    "y_pred_prob = model.predict(X_val)\n",
    "y_pred = (y_pred_prob > 0.9).astype(int)\n",
    "\n",
    "# 混淆矩陣\n",
    "cm_best = confusion_matrix(y_val, y_pred)\n",
    "print(\"混淆矩陣:\")\n",
    "print(cm_best)\n",
    "\n",
    "# 視覺化混淆矩陣\n",
    "plt.figure(figsize=(6, 4))\n",
    "sns.heatmap(cm_best, annot=True, fmt='d', cmap='Greens', xticklabels=['Non-Swallow', 'Swallow'], yticklabels=['Non-Swallow', 'Swallow'])\n",
    "plt.xlabel('Predict')\n",
    "plt.ylabel('GroundTruth')\n",
    "plt.title('confusion_matrix')\n",
    "plt.show()\n",
    "\n",
    "# 分類報告\n",
    "cr_best = classification_report(y_val, y_pred, target_names=['Non-Swallow', 'Swallow'])\n",
    "print(\"分類報告:\")\n",
    "print(cr_best)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 保存模型\n",
    "model.save('model/swallow_model.h5')\n",
    "print(\"模型已保存為 'swallow_model.h5'\")\n",
    "\n",
    "# 保存標準化器\n",
    "joblib.dump(scaler, 'scaler/scaler.pkl')\n",
    "print(\"Scaler 已保存為 'scaler.pkl'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_swallow_count(file_path, model, scaler, n_mfcc=40, hop_length=64, fixed_length=160, sr=22050, duration=0.5, overlap=0.25, threshold=0.9):\n",
    "    \"\"\"\n",
    "    預測音訊文件中的吞嚥聲數量，並防止在接下來的 0.5 秒內重複計數。\n",
    "\n",
    "    參數:\n",
    "    - file_path (str): 音訊文件路徑\n",
    "    - model (tf.keras.Model): 訓練好的模型\n",
    "    - scaler (StandardScaler): 訓練階段使用的標準化器\n",
    "    - n_mfcc (int): MFCC 的數量\n",
    "    - hop_length (int): 每次窗口移動的樣本數\n",
    "    - fixed_length (int): 固定的時間步數（橫向維度）\n",
    "    - sr (int): 採樣率\n",
    "    - duration (float): 每個片段的時長（秒）\n",
    "    - overlap (float): 每個片段之間的重疊時間（秒）\n",
    "    - threshold (float): 預測閾值\n",
    "\n",
    "    返回:\n",
    "    - count (int): 檢測到的吞嚥聲數量\n",
    "    \"\"\"\n",
    "    try:\n",
    "        y, sr = librosa.load(file_path, sr=sr)\n",
    "        \n",
    "        # 定義片段長度和步幅\n",
    "        segment_length = int(sr * duration)  # 0.5 秒\n",
    "        step = int(sr * (duration - overlap))  # 0.25 秒\n",
    "        \n",
    "        count = 0\n",
    "        last_detected = -np.inf  # 最後檢測到吞嚥聲的時間\n",
    "        \n",
    "        for start in range(0, len(y) - segment_length + 1, step):\n",
    "            end = start + segment_length\n",
    "            segment = y[start:end]\n",
    "            \n",
    "            # 提取 MFCC\n",
    "            mfcc = extract_mfcc_from_data(segment, sr, n_mfcc=n_mfcc, hop_length=hop_length, fixed_length=fixed_length)\n",
    "            if mfcc is None:\n",
    "                continue\n",
    "            \n",
    "            # 標準化\n",
    "            mfcc_reshaped = mfcc.reshape(1, -1)\n",
    "            mfcc_scaled = scaler.transform(mfcc_reshaped)\n",
    "            mfcc_scaled = mfcc_scaled.reshape(1, n_mfcc, fixed_length, 1)\n",
    "            mfcc_scaled = np.repeat(mfcc_scaled, 3, axis=-1)  # (1, 40, 160, 3)\n",
    "            \n",
    "            # 預測\n",
    "            prediction = model.predict(mfcc_scaled)[0][0]\n",
    "            current_time = start / sr  # 當前片段的開始時間（秒）\n",
    "            \n",
    "            if prediction > threshold:\n",
    "                # 檢查是否超過防止重複檢測的時間間隔\n",
    "                if current_time - last_detected >= duration:\n",
    "                    print(f\"Swallow Detected conf{prediction}\")\n",
    "                    count += 1\n",
    "                    last_detected = current_time\n",
    "        return count\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"處理文件 {file_path} 時出現錯誤: {e}\")\n",
    "        return 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 載入已保存的 scaler\n",
    "scaler = joblib.load('scaler/scaler.pkl')\n",
    "print(\"Scaler 已載入\")\n",
    "\n",
    "# 載入已保存的模型\n",
    "model = tf.keras.models.load_model('model/swallow_model.h5')\n",
    "print(\"模型已載入\")\n",
    "\n",
    "# 定義要測試的音訊文件路徑\n",
    "test_file_path = '音檔\\測試RSST.wav'  # 替換為實際路徑，注意使用正斜杠或原始字符串\n",
    "\n",
    "# 確認音訊文件存在\n",
    "if not os.path.exists(test_file_path):\n",
    "    print(f\"音訊文件 '{test_file_path}' 不存在。請確認路徑正確。\")\n",
    "else:\n",
    "    # 預測吞嚥聲數量\n",
    "    num_swallow = predict_swallow_count(\n",
    "        file_path=test_file_path,\n",
    "        model=model,\n",
    "        scaler=scaler,\n",
    "        n_mfcc=40,\n",
    "        hop_length=64,\n",
    "        fixed_length=160,\n",
    "        sr=22050,\n",
    "        duration=0.5,\n",
    "        overlap=0.25,\n",
    "        threshold=0.9  # 可以根據 ROC 曲線調整\n",
    "    )\n",
    "    print(f\"音訊文件 '{test_file_path}' 中檢測到的吞嚥聲數量: {num_swallow}\")\n",
    "\n",
    "    # # 繪製每個片段的預測概率\n",
    "    # plt.figure(figsize=(12, 4))\n",
    "    # plt.plot(predictions, marker='o', linestyle='-', label='預測概率')\n",
    "    # plt.axhline(y=0.5, color='r', linestyle='--', label='閾值 (0.5)')\n",
    "    # plt.xlabel('片段編號')\n",
    "    # plt.ylabel('預測概率')\n",
    "    # plt.title('每個片段的預測概率')\n",
    "    # plt.legend()\n",
    "    # plt.show()\n",
    "\n",
    "    # # 打印高於閾值的片段索引\n",
    "    # high_prob_indices = [i for i, prob in enumerate(predictions) if prob > 0.5]\n",
    "    # print(f\"高於閾值的片段索引: {high_prob_indices}\")\n",
    "\n",
    "    # # 載入音訊數據以進行可視化\n",
    "    # y_test, sr_test = librosa.load(test_file_path, sr=22050)\n",
    "    # for idx in high_prob_indices:\n",
    "    #     start_time = idx * (0.5 - 0.25)\n",
    "    #     end_time = start_time + 0.5\n",
    "    #     start_sample = int(sr_test * start_time)\n",
    "    #     end_sample = int(sr_test * end_time)\n",
    "    #     y_segment = y_test[start_sample:end_sample]\n",
    "\n",
    "    #     plt.figure(figsize=(10, 2))\n",
    "    #     librosa.display.waveshow(y_segment, sr=sr_test)\n",
    "    #     plt.title(f'片段 {idx} 預測概率: {predictions[idx]:.2f}')\n",
    "    #     plt.xlabel('時間 (秒)')\n",
    "    #     plt.ylabel('振幅')\n",
    "    #     plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "swallow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
