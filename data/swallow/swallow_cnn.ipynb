{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 導入必要的庫\n",
    "import os\n",
    "import numpy as np\n",
    "import librosa\n",
    "import noisereduce as nr\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Dropout, Flatten, Dense\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = 'C:/Users/bymin/OneDrive/桌面/swallow/data/'  # 請將此替換為您的實際路徑\n",
    "\n",
    "FEATURE_SAVE_PATH = 'C:/Users/bymin/OneDrive/桌面/swallow/feature'  # 請將此替換為您的實際路徑\n",
    "\n",
    "# 音頻取樣率\n",
    "SAMPLE_RATE = 16000\n",
    "\n",
    "# 梅爾頻譜圖參數\n",
    "N_MELS = 64\n",
    "N_FFT = 1024\n",
    "HOP_LENGTH = 512\n",
    "\n",
    "def load_audio(file_path, sample_rate=SAMPLE_RATE):\n",
    "    \"\"\"\n",
    "    加載音頻文件\n",
    "    \"\"\"\n",
    "    audio, sr = librosa.load(file_path, sr=sample_rate)\n",
    "    return audio, sr\n",
    "\n",
    "def normalize_audio(audio):\n",
    "    \"\"\"\n",
    "    振幅正規化，將音頻信號調整到 [-1, 1] 範圍內\n",
    "    \"\"\"\n",
    "    max_abs_amplitude = np.max(np.abs(audio))\n",
    "    if max_abs_amplitude > 0:\n",
    "        audio = audio / max_abs_amplitude\n",
    "    return audio\n",
    "\n",
    "def reduce_noise(audio, sr):\n",
    "    \"\"\"\n",
    "    降噪處理，使用 noisereduce 庫\n",
    "    \"\"\"\n",
    "    reduced_audio = nr.reduce_noise(y=audio, sr=sr)\n",
    "    return reduced_audio\n",
    "\n",
    "def extract_mel_spectrogram(audio, sr, n_mels=N_MELS, n_fft=N_FFT, hop_length=HOP_LENGTH):\n",
    "    \"\"\"\n",
    "    提取梅爾頻譜圖特徵\n",
    "    \"\"\"\n",
    "    mel_spectrogram = librosa.feature.melspectrogram(\n",
    "        y=audio,\n",
    "        sr=sr,\n",
    "        n_fft=n_fft,\n",
    "        hop_length=hop_length,\n",
    "        n_mels=n_mels\n",
    "    )\n",
    "    # 對數壓縮\n",
    "    log_mel_spectrogram = librosa.power_to_db(mel_spectrogram, ref=np.max)\n",
    "    return log_mel_spectrogram\n",
    "\n",
    "def standardize_features(feature):\n",
    "    \"\"\"\n",
    "    特徵標準化，零均值單位方差\n",
    "    \"\"\"\n",
    "    mean = np.mean(feature)\n",
    "    std = np.std(feature)\n",
    "    standardized_feature = (feature - mean) / std\n",
    "    return standardized_feature\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "總共的音頻樣本數：137\n",
      "原始標籤： ['non' 'swallow']\n",
      "編碼後的標籤： [0 1]\n"
     ]
    }
   ],
   "source": [
    "# =============================\n",
    "# 準備音頻文件路徑和標籤\n",
    "# =============================\n",
    "\n",
    "# 初始化列表\n",
    "file_paths = []\n",
    "file_labels = []\n",
    "\n",
    "# 遍歷資料夾，假設正類和負類分別存放在 'swallowing' 和 'non-swallowing' 資料夾中\n",
    "for label in ['swallow', 'non']:\n",
    "    folder_path = os.path.join(DATA_PATH, label)\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.endswith('.wav') or filename.endswith('.mp3'):\n",
    "            file_paths.append(os.path.join(folder_path, filename))\n",
    "            file_labels.append(label)\n",
    "\n",
    "# 檢查總共的樣本數量\n",
    "print(f\"總共的音頻樣本數：{len(file_paths)}\")\n",
    "label_encoder = LabelEncoder()\n",
    "encoded_labels = label_encoder.fit_transform(file_labels)\n",
    "\n",
    "# 檢查編碼後的標籤\n",
    "print(\"原始標籤：\", label_encoder.classes_)\n",
    "print(\"編碼後的標籤：\", np.unique(encoded_labels))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "已處理 10 / 137 個文件\n",
      "已處理 20 / 137 個文件\n",
      "已處理 30 / 137 個文件\n",
      "已處理 40 / 137 個文件\n",
      "已處理 50 / 137 個文件\n",
      "已處理 60 / 137 個文件\n",
      "已處理 70 / 137 個文件\n",
      "已處理 80 / 137 個文件\n",
      "已處理 90 / 137 個文件\n",
      "已處理 100 / 137 個文件\n",
      "已處理 110 / 137 個文件\n",
      "已處理 120 / 137 個文件\n",
      "已處理 130 / 137 個文件\n",
      "特徵形狀集合： {(1984,)}\n",
      "特徵矩陣形狀： (137, 1984)\n",
      "標籤向量形狀： (137,)\n"
     ]
    }
   ],
   "source": [
    "FIXED_DURATION = 1.0 # 固定時長（秒）\n",
    "FIXED_LENGTH = int(SAMPLE_RATE * FIXED_DURATION)  # 固定樣本數\n",
    "\n",
    "# ...（準備音頻文件路徑和標籤的代碼保持不變）...\n",
    "\n",
    "# 初始化列表\n",
    "features = []\n",
    "labels = []\n",
    "\n",
    "# 遍歷音頻文件，提取特徵\n",
    "for idx, (file_path, label) in enumerate(zip(file_paths, encoded_labels)):\n",
    "    try:\n",
    "        # 加載音頻\n",
    "        audio, sr = load_audio(file_path)\n",
    "        \n",
    "        # 確保音頻時長一致\n",
    "        if len(audio) < FIXED_LENGTH:\n",
    "            # 如果音頻短於固定長度，進行零填充\n",
    "            pad_width = FIXED_LENGTH - len(audio)\n",
    "            audio = np.pad(audio, (0, pad_width), mode='constant')\n",
    "        else:\n",
    "            # 如果音頻長於固定長度，進行截斷\n",
    "            audio = audio[:FIXED_LENGTH]\n",
    "        # 振幅正規化\n",
    "        audio = normalize_audio(audio)\n",
    "        # 降噪處理\n",
    "        audio = reduce_noise(audio, sr)\n",
    "        # 提取梅爾頻譜圖\n",
    "        feature = extract_mel_spectrogram(audio, sr)\n",
    "        # 標準化特徵\n",
    "        feature = standardize_features(feature)\n",
    "        # 檢查特徵形狀，確保一致\n",
    "        expected_shape = (N_MELS, int(np.ceil((FIXED_LENGTH - N_FFT) / HOP_LENGTH)) + 1)\n",
    "        if feature.shape != expected_shape:\n",
    "            # 如果形狀不一致，進行修正\n",
    "            feature = librosa.util.fix_length(feature, size=expected_shape[1], axis=1)\n",
    "        # 展平成一維向量\n",
    "        feature = feature.flatten()\n",
    "        # 添加到列表\n",
    "        features.append(feature)\n",
    "        labels.append(label)\n",
    "        \n",
    "        # 進度輸出\n",
    "        if (idx + 1) % 10 == 0:\n",
    "            print(f\"已處理 {idx + 1} / {len(file_paths)} 個文件\")\n",
    "                \n",
    "    except Exception as e:\n",
    "        print(f\"處理文件 {file_path} 時發生錯誤：{e}\")\n",
    "\n",
    "feature_shapes = [feature.shape for feature in features]\n",
    "unique_shapes = set(feature_shapes)\n",
    "\n",
    "print(\"特徵形狀集合：\", unique_shapes)\n",
    "\n",
    "# 確保只有一種特徵形狀\n",
    "if len(unique_shapes) == 1:\n",
    "    # 轉換為 NumPy 陣列\n",
    "    features = np.array(features)\n",
    "    labels = np.array(labels)\n",
    "    \n",
    "    print(\"特徵矩陣形狀：\", features.shape)\n",
    "    print(\"標籤向量形狀：\", labels.shape)\n",
    "else:\n",
    "    print(\"特徵形狀不一致，請檢查預處理步驟。\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "特徵矩陣形狀： (137, 1984)\n",
      "標籤向量形狀： (137,)\n"
     ]
    }
   ],
   "source": [
    "# =============================\n",
    "# 將列表轉換為 NumPy 陣列\n",
    "# =============================\n",
    "\n",
    "features = np.array(features)\n",
    "labels = np.array(labels)\n",
    "\n",
    "# 檢查形狀\n",
    "print(\"特徵矩陣形狀：\", features.shape)\n",
    "print(\"標籤向量形狀：\", labels.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "訓練集特徵形狀： (95, 1984)\n",
      "訓練集標籤形狀： (95,)\n",
      "驗證集特徵形狀： (21, 1984)\n",
      "驗證集標籤形狀： (21,)\n",
      "測試集特徵形狀： (21, 1984)\n",
      "測試集標籤形狀： (21,)\n"
     ]
    }
   ],
   "source": [
    "# =============================\n",
    "# 劃分資料集\n",
    "# =============================\n",
    "\n",
    "# 首先劃分訓練集和臨時集（驗證集 + 測試集）\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "    features, labels, test_size=0.3, random_state=42, stratify=labels)\n",
    "\n",
    "# 然後劃分驗證集和測試集\n",
    "X_val, X_test, y_val, y_test = train_test_split(\n",
    "    X_temp, y_temp, test_size=0.5, random_state=42, stratify=y_temp)\n",
    "\n",
    "# 檢查各個資料集的形狀\n",
    "print(\"訓練集特徵形狀：\", X_train.shape)\n",
    "print(\"訓練集標籤形狀：\", y_train.shape)\n",
    "print(\"驗證集特徵形狀：\", X_val.shape)\n",
    "print(\"驗證集標籤形狀：\", y_val.shape)\n",
    "print(\"測試集特徵形狀：\", X_test.shape)\n",
    "print(\"測試集標籤形狀：\", y_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "特徵形狀: (137, 1984)\n",
      "標籤形狀: (184,)\n",
      "標準化後的特徵形狀: (137, 1984)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [137, 184]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[108], line 14\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m標準化後的特徵形狀: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mX\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# 分割訓練集和測試集\u001b[39;00m\n\u001b[1;32m---> 14\u001b[0m X_train, X_test, y_train, y_test \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_test_split\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstratify\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m42\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m訓練集大小: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mX_train\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m測試集大小: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mX_test\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\bymin\\anaconda3\\envs\\swallow\\lib\\site-packages\\sklearn\\utils\\_param_validation.py:213\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    207\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    208\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m    209\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m    210\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m    211\u001b[0m         )\n\u001b[0;32m    212\u001b[0m     ):\n\u001b[1;32m--> 213\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    214\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    215\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[0;32m    217\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[0;32m    219\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[0;32m    220\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    221\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    222\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[0;32m    223\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\bymin\\anaconda3\\envs\\swallow\\lib\\site-packages\\sklearn\\model_selection\\_split.py:2782\u001b[0m, in \u001b[0;36mtrain_test_split\u001b[1;34m(test_size, train_size, random_state, shuffle, stratify, *arrays)\u001b[0m\n\u001b[0;32m   2779\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n_arrays \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m   2780\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAt least one array required as input\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m-> 2782\u001b[0m arrays \u001b[38;5;241m=\u001b[39m \u001b[43mindexable\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43marrays\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2784\u001b[0m n_samples \u001b[38;5;241m=\u001b[39m _num_samples(arrays[\u001b[38;5;241m0\u001b[39m])\n\u001b[0;32m   2785\u001b[0m n_train, n_test \u001b[38;5;241m=\u001b[39m _validate_shuffle_split(\n\u001b[0;32m   2786\u001b[0m     n_samples, test_size, train_size, default_test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.25\u001b[39m\n\u001b[0;32m   2787\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\bymin\\anaconda3\\envs\\swallow\\lib\\site-packages\\sklearn\\utils\\validation.py:514\u001b[0m, in \u001b[0;36mindexable\u001b[1;34m(*iterables)\u001b[0m\n\u001b[0;32m    484\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Make arrays indexable for cross-validation.\u001b[39;00m\n\u001b[0;32m    485\u001b[0m \n\u001b[0;32m    486\u001b[0m \u001b[38;5;124;03mChecks consistent length, passes through None, and ensures that everything\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    510\u001b[0m \u001b[38;5;124;03m[[1, 2, 3], array([2, 3, 4]), None, <...Sparse...dtype 'int64'...shape (3, 1)>]\u001b[39;00m\n\u001b[0;32m    511\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    513\u001b[0m result \u001b[38;5;241m=\u001b[39m [_make_indexable(X) \u001b[38;5;28;01mfor\u001b[39;00m X \u001b[38;5;129;01min\u001b[39;00m iterables]\n\u001b[1;32m--> 514\u001b[0m \u001b[43mcheck_consistent_length\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    515\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[1;32mc:\\Users\\bymin\\anaconda3\\envs\\swallow\\lib\\site-packages\\sklearn\\utils\\validation.py:457\u001b[0m, in \u001b[0;36mcheck_consistent_length\u001b[1;34m(*arrays)\u001b[0m\n\u001b[0;32m    455\u001b[0m uniques \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39munique(lengths)\n\u001b[0;32m    456\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(uniques) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m--> 457\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    458\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound input variables with inconsistent numbers of samples: \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    459\u001b[0m         \u001b[38;5;241m%\u001b[39m [\u001b[38;5;28mint\u001b[39m(l) \u001b[38;5;28;01mfor\u001b[39;00m l \u001b[38;5;129;01min\u001b[39;00m lengths]\n\u001b[0;32m    460\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [137, 184]"
     ]
    }
   ],
   "source": [
    "# 將特徵和標籤轉換為NumPy數組\n",
    "X = np.array(features)\n",
    "y = np.array(labels_processed)\n",
    "\n",
    "print(f\"特徵形狀: {X.shape}\")\n",
    "print(f\"標籤形狀: {y.shape}\")\n",
    "\n",
    "# 將數值標準化到0-1之間\n",
    "X = X / np.max(X)\n",
    "\n",
    "print(f\"標準化後的特徵形狀: {X.shape}\")\n",
    "\n",
    "# 分割訓練集和測試集\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)\n",
    "\n",
    "print(f\"訓練集大小: {X_train.shape}\")\n",
    "print(f\"測試集大小: {X_test.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "原始標籤分佈: Counter({0: 135, 1: 49})\n",
      "分割前標籤分佈: Counter({0: 135, 1: 49})\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "# 檢查原始標籤分佈\n",
    "print(\"原始標籤分佈:\", Counter(y))\n",
    "\n",
    "# 分割前後標籤的分佈\n",
    "X = np.array(features)\n",
    "y = np.array(labels_processed)\n",
    "\n",
    "# 確認標籤分佈\n",
    "label_counts = Counter(y)\n",
    "print(\"分割前標籤分佈:\", label_counts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "特徵形狀: (2, 128, 87, 1)\n",
      "標籤形狀: (2,)\n"
     ]
    }
   ],
   "source": [
    "# 將特徵和標籤轉換為numpy數組\n",
    "X = np.array(features)\n",
    "y = np.array(labels_list)\n",
    "\n",
    "print(f\"特徵形狀: {X.shape}\")\n",
    "print(f\"標籤形狀: {y.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "訓練集大小: (1, 128, 87, 1)\n",
      "測試集大小: (1, 128, 87, 1)\n"
     ]
    }
   ],
   "source": [
    "# 分割訓練集和測試集\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "print(f\"訓練集大小: {X_train.shape}\")\n",
    "print(f\"測試集大小: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "訓練集標籤範例: [[1. 0.]]\n",
      "測試集標籤範例: [[1. 0.]]\n"
     ]
    }
   ],
   "source": [
    "# 將標籤轉為類別\n",
    "y_train = to_categorical(y_train, num_classes=2)\n",
    "y_test = to_categorical(y_test, num_classes=2)\n",
    "\n",
    "print(f\"訓練集標籤範例: {y_train[:5]}\")\n",
    "print(f\"測試集標籤範例: {y_test[:5]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "訓練集類別分佈: Counter({0: 1})\n",
      "驗證集類別分佈: Counter({0: 1})\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# 獲取訓練集和驗證集的真實類別\n",
    "y_train_classes = np.argmax(y_train, axis=1)\n",
    "y_test_classes = np.argmax(y_test, axis=1)\n",
    "\n",
    "# 計算類別分佈\n",
    "train_counter = Counter(y_train_classes)\n",
    "test_counter = Counter(y_test_classes)\n",
    "\n",
    "print(f\"訓練集類別分佈: {train_counter}\")  # 例如: Counter({0: 800, 1: 600})\n",
    "print(f\"驗證集類別分佈: {test_counter}\")  # 例如: Counter({0: 200, 1: 150})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputShape:(128, 87, 1)\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 126, 85, 32)       320       \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 63, 42, 32)       0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 61, 40, 64)        18496     \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, 30, 20, 64)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 28, 18, 128)       73856     \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPooling  (None, 14, 9, 128)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 16128)             0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 128)               2064512   \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 128)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 2)                 258       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,157,442\n",
      "Trainable params: 2,157,442\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def build_cnn(input_shape):\n",
    "    \"\"\"\n",
    "    建立CNN模型架構。\n",
    "\n",
    "    參數:\n",
    "    - input_shape: 輸入數據的形狀，例如 (128, 130, 1)\n",
    "\n",
    "    返回:\n",
    "    - model: 建立好的Keras模型\n",
    "    \"\"\"\n",
    "    model = Sequential()\n",
    "\n",
    "    # 第一個卷積層\n",
    "    model.add(Conv2D(32, (3, 3), activation='relu', input_shape=input_shape))\n",
    "    model.add(MaxPooling2D((2, 2)))\n",
    "    \n",
    "    # 第二個卷積層\n",
    "    model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "    model.add(MaxPooling2D((2, 2)))\n",
    "    \n",
    "    # 第三個卷積層\n",
    "    model.add(Conv2D(128, (3, 3), activation='relu'))\n",
    "    model.add(MaxPooling2D((2, 2)))\n",
    "    \n",
    "    # 展平層\n",
    "    model.add(Flatten())\n",
    "    \n",
    "    # 全連接層\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    \n",
    "    # 輸出層\n",
    "    model.add(Dense(2, activation='softmax'))\n",
    "    \n",
    "    return model\n",
    "\n",
    "# 獲取輸入形狀\n",
    "input_shape = X_train.shape[1:]  # 例如 (128, 130, 1)\n",
    "print(f\"inputShape:{input_shape}\")\n",
    "\n",
    "# 建立模型\n",
    "model = build_cnn(input_shape)\n",
    "\n",
    "# 編譯模型\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# 顯示模型摘要\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 2/50\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 3/50\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 4/50\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 5/50\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 6/50\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 7/50\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 8/50\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 9/50\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 10/50\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 11/50\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 12/50\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 13/50\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 14/50\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 15/50\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 16/50\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 17/50\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 18/50\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 19/50\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 20/50\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 21/50\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 22/50\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 23/50\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 24/50\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 25/50\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 26/50\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 27/50\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 28/50\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 29/50\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 30/50\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 31/50\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 32/50\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 33/50\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 34/50\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 35/50\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 36/50\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 37/50\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 38/50\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 39/50\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 40/50\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 41/50\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 42/50\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 43/50\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 44/50\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 45/50\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 46/50\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 47/50\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 48/50\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 49/50\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 50/50\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n"
     ]
    }
   ],
   "source": [
    "# 定義早停回調，以防止過度擬合\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "\n",
    "# 訓練模型\n",
    "history = model.fit(X_train, y_train, epochs=50, batch_size=32,\n",
    "                    validation_data=(X_test, y_test))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "swallow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
